{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad8dde58-083a-48c7-8951-b4887a305837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656a57ec-5678-4c18-b020-f6d224d02677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_kernel(T, alpha):\n",
    "    \"\"\"\n",
    "    Computes exponential kernel weights at time T.\n",
    "    \n",
    "    Parameters:\n",
    "        T (int): Time index to evaluate weights at (typically T = total sample length)\n",
    "        alpha (float): Exponential decay parameter (0 < alpha < 1)\n",
    "    \n",
    "    Returns:\n",
    "        weights (T,): Kernel weights K_{s,T} for s = 1 to T\n",
    "    \"\"\"\n",
    "    s_vals = np.arange(1, T + 1)\n",
    "    weights = alpha**(T - s_vals)\n",
    "    normalized_weights = (1 - alpha) * weights / (1 - alpha**T)\n",
    "    return normalized_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6361eb75-c3cc-4718-8d67-05decd99b7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.96046483e-08 1.19209297e-07 2.38418593e-07 4.76837187e-07\n",
      " 9.53674373e-07 1.90734875e-06 3.81469749e-06 7.62939499e-06\n",
      " 1.52587900e-05 3.05175799e-05 6.10351599e-05 1.22070320e-04\n",
      " 2.44140640e-04 4.88281279e-04 9.76562558e-04 1.95312512e-03\n",
      " 3.90625023e-03 7.81250047e-03 1.56250009e-02 3.12500019e-02\n",
      " 6.25000037e-02 1.25000007e-01 2.50000015e-01 5.00000030e-01]\n"
     ]
    }
   ],
   "source": [
    "T = 24       # T is the estimation window (the number of months) that we consider in local PCA\n",
    "alpha = 0.5  # alpha is the exponential decay parameter\n",
    "exp_decay_weights = exponential_kernel(T, alpha)\n",
    "print(exp_decay_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c2f8c2a-d4de-4560-afec-1439fb37e2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1131/3167354070.py:50: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: np.sum((g['w'] / g['w'].sum()) * g['Ret_next']))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "RAW_CSV      = \"./Imputed _characteristics.csv\"\n",
    "SAVE_DIR     = Path(\"./cml_outputs\"); SAVE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "TRAIN_END    = 201612\n",
    "OOS_START    = 200001\n",
    "OOS_END      = 202012 \n",
    "EPOCHS_DNN   = 3\n",
    "HIDDEN1, HIDDEN2 = 32, 16\n",
    "LR           = 1e-4\n",
    "BATCH_SIZE   = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42); np.random.seed(42)\n",
    "\n",
    "full_df = pd.read_csv(RAW_CSV)\n",
    "full_df = full_df.rename(columns={'return': 'Ret'})\n",
    "# 截断离群点\n",
    "full_df['Ret'] = full_df['Ret'].clip(lower=full_df['Ret'].quantile(0.01), upper=full_df['Ret'].quantile(0.99))\n",
    "\n",
    "full_df['date_clean'] = full_df['date'].astype(str).str.split('.').str[0].str.zfill(8)\n",
    "full_df['yyyymm'] = (\n",
    "    pd.to_datetime(full_df['date_clean'], format='%Y%m%d')\n",
    "      .dt.to_period('M')\n",
    "      .astype(str)\n",
    "      .str.replace(\"-\", \"\")\n",
    "      .astype(int)\n",
    ")\n",
    "full_df.drop(columns=['date_clean'], inplace=True)\n",
    "df = full_df.sort_values(['permno', 'yyyymm']).copy()\n",
    "\n",
    "feature_cols = df.select_dtypes('number').columns.difference(\n",
    "    ['permno', 'date', 'yyyymm', 'Ret', 'B2M', 'ME']\n",
    ").tolist()\n",
    "\n",
    "lag_cols = [f\"{c}_lag\" for c in feature_cols]\n",
    "df[lag_cols] = df.groupby('permno')[feature_cols].shift(1)\n",
    "lag_features = lag_cols\n",
    "\n",
    "df['w'] = df['ME'] / df.groupby('yyyymm')['ME'].transform('sum')\n",
    "\n",
    "df['Ret_next'] = df.groupby('permno')['Ret'].shift(-1)\n",
    "\n",
    "# 删除为nan值的行\n",
    "df = df.dropna(subset=['Ret_next', 'w']).copy()\n",
    "\n",
    "df_valid = df.dropna(subset=['Ret_next', 'w']).copy()\n",
    "\n",
    "index_returns = (\n",
    "    df_valid.groupby('yyyymm')\n",
    "            .apply(lambda g: np.sum((g['w'] / g['w'].sum()) * g['Ret_next']))\n",
    "            .reset_index().rename(columns={0: 'y_next'})\n",
    ")\n",
    "index_returns.to_csv(SAVE_DIR / \"index_returns_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bd28101-d4e8-4369-a3d3-47c1d4eeacbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Months: 100%|██████████| 480/480 [29:37<00:00,  3.70s/it]\n"
     ]
    }
   ],
   "source": [
    "# I–III\n",
    "train_df = df[df['yyyymm'] <= TRAIN_END].copy()\n",
    "g_list_train = []\n",
    "predicted_returns = []\n",
    "t = 0\n",
    "all_xhat_list = []   # Save (permno, yyyymm, predicted_x)\n",
    "all_beta_dicts = []   # Save (permno, yyyymm, beta_1…beta_K_g)\n",
    "all_g_dicts = []      # Save (yyyymm, g_1…g_K_g)\n",
    "K_g = 5\n",
    "G_train = []\n",
    "# pca_file = 'pca_outputs/'\n",
    "# beta_records = []\n",
    "for m in tqdm(sorted(train_df['yyyymm'].unique()), desc=\"Months\"):\n",
    "    dm = train_df[(train_df['yyyymm'] == m)].dropna(subset=['Ret_next'] + lag_features)\n",
    "    if dm.empty:\n",
    "        continue\n",
    "\n",
    "    # Step I: DNN \n",
    "    X = dm[lag_features].values; y = dm['Ret_next'].values\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32).view(-1,1).to(device)\n",
    "\n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(X.shape[1], HIDDEN1), nn.ReLU(),\n",
    "        nn.Linear(HIDDEN1, HIDDEN2), nn.ReLU(),\n",
    "        nn.Linear(HIDDEN2, 1)\n",
    "    ).to(device)\n",
    "    opt = optim.Adam(net.parameters(), lr=LR)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    net.train()\n",
    "    loader = DataLoader(TensorDataset(X_tensor, y_tensor),\n",
    "                        batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    for epoch in range(EPOCHS_DNN):\n",
    "        for xb, yb in loader:\n",
    "            opt.zero_grad()\n",
    "            loss = loss_fn(net(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = net(X_tensor).squeeze().cpu().numpy()\n",
    "        df_month = dm.copy()\n",
    "        df_month['predicted_x'] = preds\n",
    "        predicted_returns.append(df_month[['permno', 'yyyymm', 'Ret_next', 'predicted_x'] + list(feature_cols)])\n",
    "        \n",
    "    # xhat_df = df_month[['permno', 'yyyymm', 'predicted_x', 'Ret_next']].copy()\n",
    "    # all_xhat_list.append(xhat_df)\n",
    "    \n",
    "    # step 2 local PCA\n",
    "    t_start = max(0, t - T + 1)\n",
    "    t += 1\n",
    "    df_window = predicted_returns[t_start:(t+1)]\n",
    "    num_ts = len(df_window)\n",
    "    weights = exp_decay_weights[-num_ts:]\n",
    "    w = weights / weights.sum()\n",
    "\n",
    "    # Merge all months in the window to form X matrix \n",
    "    # df_months = [df[['permno', 'predicted_x']] for df in df_window]\n",
    "\n",
    "    # Rename prevent join error\n",
    "    df_months = [\n",
    "    df_t[['permno', 'predicted_x']].rename(columns={'predicted_x': f'pred_{i}'})\n",
    "    for i, df_t in enumerate(df_window)\n",
    "]\n",
    "    X_df = df_months[0]\n",
    "    for df_t in df_months[1:]:\n",
    "        X_df = pd.merge(X_df, df_t, on='permno')\n",
    "    \n",
    "    permnos = X_df['permno'].values\n",
    "    X_mat = X_df.drop(columns=['permno']).values # shape: N x num_ts\n",
    "\n",
    "    # =============== PCA ===============\n",
    "    # Center across time using weighted mean\n",
    "    X_mean = np.average(X_mat, axis=1, weights=w)[:, None]\n",
    "    X_centered = X_mat - X_mean\n",
    "\n",
    "    # Compute weighted cross-sectional covariance: Σ = X W X^T\n",
    "    X_weighted = X_centered * np.sqrt(w)[None, :]\n",
    "    X_cov = X_weighted @ X_weighted.T  # N x N\n",
    "\n",
    "    # Eigendecomposition\n",
    "    eigvals, eigvecs = np.linalg.eigh(X_cov)\n",
    "    idx = np.argsort(eigvals)[::-1]  # descending order\n",
    "    eigvals = eigvals[idx]\n",
    "    eigvecs = eigvecs[:, idx]\n",
    "\n",
    "    # Top K_g eigenvectors: asset loadings (\"betas\")\n",
    "    components = eigvecs[:, :K_g]  # N x K_g\n",
    "    for i, p in enumerate(permnos):\n",
    "        all_beta_dicts.append({\n",
    "            'permno': p,\n",
    "            'yyyymm': m,\n",
    "            'beta': components[i, :].copy()  # Length K_g NumPy array\n",
    "        })\n",
    "    v = np.array(pd.merge(dm, X_df, on='permno')['B2M']) # Length N vector\n",
    "    g = (components.T @ v) / len(v) # g K_g vector\n",
    "    G_train.append({'yyyymm': m, 'g_t': g})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d5c35f-8dd4-4827-9d50-7d8b70344886",
   "metadata": {},
   "source": [
    "### 分为两个表，一个是 \n",
    "1. permno, yyyymm, x_hat, Ret_next\n",
    "2. yyyymm, g_t, y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf76eaf2-718d-49fe-9f37-f114cadf60fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat_all = pd.concat(predicted_returns, ignore_index=True)\n",
    "xhat_all = xhat_all[['permno', 'yyyymm', 'Ret_next', 'predicted_x']]\n",
    "beta_all = pd.DataFrame(all_beta_dicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d890441-ffe2-4f6d-9153-bbbb389ed6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_px_beta = pd.merge(\n",
    "    xhat_all,\n",
    "    beta_all,\n",
    "    on=['permno', 'yyyymm'],\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7457113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In‐Sample x_hat\n",
    "merged_px_beta.to_csv(SAVE_DIR/\"xhat_train.csv\", index=False)\n",
    "\n",
    "gdf_train = pd.DataFrame(G_train)\n",
    "gdf_train.to_csv(SAVE_DIR/\"g_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04a04779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== In‐Sample OLS Summary ===\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 y_next   R-squared:                       0.012\n",
      "Model:                            OLS   Adj. R-squared:                  0.002\n",
      "Method:                 Least Squares   F-statistic:                     1.083\n",
      "Date:                Sun, 08 Jun 2025   Prob (F-statistic):              0.369\n",
      "Time:                        03:23:34   Log-Likelihood:                -911.60\n",
      "No. Observations:                 479   AIC:                             1835.\n",
      "Df Residuals:                     473   BIC:                             1860.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:                  HAC                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.1432      0.074      1.928      0.054      -0.002       0.289\n",
      "x1           216.1975    122.682      1.762      0.078     -24.254     456.649\n",
      "x2          -133.3499    123.707     -1.078      0.281    -375.811     109.111\n",
      "x3           -16.4739    124.227     -0.133      0.895    -259.954     227.007\n",
      "x4           -55.4986    107.878     -0.514      0.607    -266.936     155.939\n",
      "x5          -116.2343    138.415     -0.840      0.401    -387.523     155.055\n",
      "==============================================================================\n",
      "Omnibus:                      148.416   Durbin-Watson:                   2.043\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2728.035\n",
      "Skew:                          -0.834   Prob(JB):                         0.00\n",
      "Kurtosis:                      14.572   Cond. No.                     1.79e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 3 lags and without small sample correction\n",
      "[2] The condition number is large, 1.79e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# ==================== STEP 3: IN‐SAMPLE OLS ====================\n",
    "\n",
    "idx_ret_train = index_returns[index_returns['yyyymm'] <= TRAIN_END]\n",
    "reg_train = pd.merge(gdf_train, idx_ret_train, on='yyyymm', how='inner').dropna()\n",
    "\n",
    "X_tr = sm.add_constant(reg_train['g_t'].tolist())\n",
    "Y_tr = reg_train['y_next']\n",
    "ols = sm.OLS(Y_tr, X_tr).fit(cov_type='HAC', cov_kwds={'maxlags':3})\n",
    "print(\"\\n=== In‐Sample OLS Summary ===\")\n",
    "print(ols.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33d27481",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array(ols.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd42f3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Months: 100%|██████████| 251/251 [3:20:46<00:00, 48.00s/it]  \n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "idx_ret_full = index_returns.set_index('yyyymm')\n",
    "predicted_returns = []\n",
    "\n",
    "xhat_oos_list = []\n",
    "oos_train_df = df[(df['yyyymm'] <= OOS_END) & (df['yyyymm'] >= OOS_START)].copy()\n",
    "g_list_train = []\n",
    "predicted_returns = []\n",
    "t = 0\n",
    "all_xhat_list = []   # Save (permno, yyyymm, predicted_x)\n",
    "all_beta_dicts = []   # Save (permno, yyyymm, beta_1…beta_K_g)\n",
    "all_g_dicts = []      # Save (yyyymm, g_1…g_K_g)\n",
    "K_g = 5\n",
    "G_train = []\n",
    "# pca_file = 'pca_outputs/'\n",
    "# beta_records = []\n",
    "for m in tqdm(sorted(oos_train_df['yyyymm'].unique()), desc=\"Months\"):\n",
    "    if m < OOS_START or m > OOS_END:\n",
    "        continue\n",
    "    df_past = oos_train_df[oos_train_df['yyyymm'] <= m].copy()\n",
    "    dm = df_past[df_past['yyyymm'] == m].dropna(subset=['Ret'] + lag_features)\n",
    "    if dm.empty:\n",
    "        continue\n",
    "    # print(\"****\")\n",
    "    # Step I: DNN 回归\n",
    "    hist = df_past.dropna(subset=['Ret'] + lag_features)\n",
    "    X_hist = hist[lag_features].values\n",
    "    y_hist = hist['Ret'].values\n",
    "\n",
    "    scaler_m = StandardScaler().fit(X_hist)\n",
    "    X_hist_scaled = scaler_m.transform(X_hist)\n",
    "    X_hist_tensor = torch.tensor(X_hist_scaled, dtype=torch.float32).to(device)\n",
    "    y_hist_tensor = torch.tensor(y_hist, dtype=torch.float32).view(-1,1).to(device)\n",
    "    net_m = nn.Sequential(\n",
    "        nn.Linear(X.shape[1], HIDDEN1), nn.ReLU(),\n",
    "        nn.Linear(HIDDEN1, HIDDEN2), nn.ReLU(),\n",
    "        nn.Linear(HIDDEN2, 1)\n",
    "    ).to(device)\n",
    "    opt_m = optim.Adam(net_m.parameters(), lr=LR)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    net_m.train()\n",
    "    loader_m = DataLoader(TensorDataset(X_hist_tensor, y_hist_tensor),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for epoch in range(EPOCHS_DNN):\n",
    "        for xb, yb in loader_m:\n",
    "            opt_m.zero_grad()\n",
    "            loss = loss_fn(net_m(xb), yb)\n",
    "            loss.backward()\n",
    "            opt_m.step()\n",
    "\n",
    "    # Predict x_hat for month m\n",
    "    X_m = dm[lag_features].values\n",
    "    X_m_scaled = scaler_m.transform(X_m)\n",
    "    X_m_tensor = torch.tensor(X_m_scaled, dtype=torch.float32).to(device)\n",
    "    net_m.eval()\n",
    "    with torch.no_grad():\n",
    "        x_hat_m = net_m(X_m_tensor).squeeze().cpu().numpy()\n",
    "        df_month = dm.copy()\n",
    "        df_month['predicted_x'] = x_hat_m\n",
    "        predicted_returns.append(df_month[['permno', 'yyyymm', 'Ret_next', 'predicted_x'] + list(feature_cols)])\n",
    "        \n",
    "    # ——— 新增：把当月的 OOS x_hat_m 及对应 permno、yyyymm 存到列表里 ———\n",
    "    tmp_oos = dm[['permno']].reset_index(drop=True).copy()\n",
    "    tmp_oos['yyyymm'] = m\n",
    "    tmp_oos['x_hat'] = x_hat_m\n",
    "    xhat_oos_list.append(tmp_oos)\n",
    "    # xhat_df = df_month[['permno', 'yyyymm', 'predicted_x', 'Ret_next']].copy()\n",
    "    # all_xhat_list.append(xhat_df)\n",
    "    \n",
    "    # step 2 local PCA\n",
    "    t_start = max(0, t - T + 1)\n",
    "    t += 1\n",
    "    df_window = predicted_returns[t_start:(t+1)]\n",
    "    num_ts = len(df_window)\n",
    "    weights = exp_decay_weights[-num_ts:]\n",
    "    w = weights / weights.sum()\n",
    "\n",
    "    # Merge all months in the window to form X matrix \n",
    "    # df_months = [df[['permno', 'predicted_x']] for df in df_window]\n",
    "\n",
    "    # 重命名避免join时候出错\n",
    "    df_months = [\n",
    "    df[['permno', 'predicted_x']].rename(columns={'predicted_x': f'pred_{i}'})\n",
    "    for i, df in enumerate(df_window)\n",
    "]\n",
    "    X_df = df_months[0]\n",
    "    for df_ in df_months[1:]:\n",
    "        X_df = pd.merge(X_df, df_, on='permno')\n",
    "    \n",
    "    permnos = X_df['permno'].values\n",
    "    X_mat = X_df.drop(columns=['permno']).values # shape: N x num_ts\n",
    "\n",
    "    # =============== PCA ===============\n",
    "    # Center across time using weighted mean\n",
    "    X_mean = np.average(X_mat, axis=1, weights=w)[:, None]\n",
    "    X_centered = X_mat - X_mean\n",
    "\n",
    "    # Compute weighted cross-sectional covariance: Σ = X W X^T\n",
    "    X_weighted = X_centered * np.sqrt(w)[None, :]\n",
    "    X_cov = X_weighted @ X_weighted.T  # N x N\n",
    "\n",
    "    # Eigendecomposition\n",
    "    eigvals, eigvecs = np.linalg.eigh(X_cov)\n",
    "    idx = np.argsort(eigvals)[::-1]  # descending order\n",
    "    eigvals = eigvals[idx]\n",
    "    eigvecs = eigvecs[:, idx]\n",
    "\n",
    "    # Top K_g eigenvectors: asset loadings (\"betas\")\n",
    "    components = eigvecs[:, :K_g]  # N x K_g\n",
    "    for i, p in enumerate(permnos):\n",
    "        all_beta_dicts.append({\n",
    "            'permno': p,\n",
    "            'yyyymm': m,\n",
    "            'beta': components[i, :].copy()  # Length K_g NumPy array\n",
    "        })\n",
    "    v = np.array(pd.merge(dm, X_df, on='permno')['B2M']) # N\n",
    "    g = (components.T @ v) / len(v) # g is K_g vector\n",
    "    G_train.append({'yyyymm': m, 'g_t': g})\n",
    "\n",
    "\n",
    "    y_hat = np.insert(g, 0, 1) @ c.T\n",
    "\n",
    "    # # True y_{m+1}\n",
    "    if m not in idx_ret_full.index:\n",
    "        continue\n",
    "    y_true = float(idx_ret_full.loc[m, 'y_next'])\n",
    "\n",
    "    results.append({\n",
    "        'yyyymm': m,\n",
    "        'g_t': g,\n",
    "        'y_true': y_true,\n",
    "        'y_hat': y_hat \n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76b81370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   → oos_results.csv saved.\n",
      "\n",
      "=== OOS Validation Set(200001–202012) ===\n",
      "OOS-MSE   : 4.848314e+00\n",
      "OOS-R²    : -0.0136\n",
      "Hit-Rate  : 0.5857\n"
     ]
    }
   ],
   "source": [
    "# Save All OOS x_hat\n",
    "all_xhat_oos = pd.concat(xhat_oos_list, ignore_index=True)\n",
    "all_xhat_oos.to_csv(SAVE_DIR/\"xhat_oos.csv\", index=False)\n",
    "\n",
    "# Save OOS prediction result\n",
    "res_df = pd.DataFrame(results)\n",
    "res_df.to_csv(SAVE_DIR/\"oos_results.csv\", index=False)\n",
    "print(\"   → oos_results.csv saved.\")\n",
    "\n",
    "# Compute OOS metric\n",
    "mse = np.mean((res_df['y_true'] - res_df['y_hat'])**2)\n",
    "y_bar = Y_tr.mean()\n",
    "mse_bench = np.mean((res_df['y_true'] - y_bar)**2)\n",
    "r2_oos = 1 - mse / mse_bench\n",
    "hit  = np.mean(np.sign(res_df['y_true']) == np.sign(res_df['y_hat']))\n",
    "\n",
    "print(f\"\\n=== OOS Validation Set({OOS_START}–{OOS_END}) ===\")\n",
    "print(f\"OOS-MSE   : {mse:.6e}\")\n",
    "print(f\"OOS-R²    : {r2_oos:.4f}\")\n",
    "print(f\"Hit-Rate  : {hit:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d573eda-6e71-43da-bb2e-fc189ca17131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
